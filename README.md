# ğŸš€ WordPiece Tokenizer 

## ğŸ“Œ Overview
This project implements a **WordPiece Tokenizer from scratch**, allowing users to specify the vocabulary size. 
### ğŸ”¥ Key Features:
- Customizable vocabulary size
- Efficient tokenization using subword merging
- Stopword removal and text preprocessing
- Outputs tokenized sentences in JSON format
- Saves vocabulary for reuse

---

## ğŸ“– Task 1: WordPiece Tokenizer Implementation

### ğŸ› ï¸ 1. Preprocessing
- **Lowercasing** â€“ Ensures uniformity and removes case-sensitive discrepancies.
- **Whitespace Normalization** â€“ Eliminates leading/trailing spaces and replaces multiple spaces with a single space.
- **Special Character Removal** â€“ Removes punctuation and symbols.
- **Stopword Removal** â€“ Filters out common words like *the, and, in, of, for*.

---

### ğŸ“œ 2. Vocabulary Construction
- **Initialization** â€“ The vocabulary starts with special tokens `[PAD]` and `[UNK]`.
- **Character Breakdown** â€“ Each word is split into individual characters.
- **Subword Identification**:
  - Frequent adjacent character pairs are identified.
  - The most frequent pair is merged into a new subword.
  - This process continues until the vocabulary size is reached.

- **Example Vocabulary Tokens:**
  ```
  [PAD]
  [UNK]
  ##ing
  abou
  ##el
  ##eling
  fe
  feel
  feeling
  ```

- **Final vocabulary is saved in** `vocabulary.txt`.

---

### ğŸ”— 3. Tokenization
- **Token Mapping** â€“ Words are converted into subword representations.
- **Longest-Match-First Principle**:
  - The longest subword match from the vocabulary is selected.
  - When a word is split, all subwords except the first are prefixed with `"##"`.
- **Handling Unknown Words**:
  - If a word is not found in the vocabulary, it is decomposed into known subwords.
  - If no valid subwords exist, it is replaced with `[UNK]`.

- **Example Tokenization Output:**
  ```
  Input: "I am feeling great"
  Tokenized: ['I', 'am', 'feeling', '##ing', 'great']
  ```
- **Tokenized sentences are saved in** `tokenized_data.json`.

---

## âš™ï¸ How to Use
### 1ï¸âƒ£ Run the Tokenizer
```sh
python tokenizer.py
```
- You will be prompted to enter a vocabulary size.  
  *(Default is 1000 if no input is given.)*

### 2ï¸âƒ£ Input Files:
- `corpus.txt` â€“ The raw text corpus used to construct the vocabulary.
- `sample_test.json` â€“ JSON file containing test sentences to tokenize.

### 3ï¸âƒ£ Output Files:
- `vocabulary.txt` â€“ The generated vocabulary.
- `tokenized_data.json` â€“ Tokenized text results.

---

## ğŸ“Œ Problem Statement
Implement a WordPiece Tokenizer from scratch, allowing a user-defined vocabulary size. Construct a vocabulary and apply it to text tokenization.

---

## ğŸ“ Example JSON Input/Output
### ğŸ”¹ Input (`sample_test.json`):
```json
[
    {"id": 1, "sentence": "I am feeling happy today"},
    {"id": 2, "sentence": "Machine learning is exciting"}
]
```

### ğŸ”¹ Output (`tokenized_data.json`):
```json
[
    {"id": 1, "tokens": ["I", "am", "feeling", "##ing", "happy", "today"]},
    {"id": 2, "tokens": ["Machine", "learning", "is", "exciting"]}
]
```

---

## ğŸ“Œ Requirements
- Python 3.x
- JSON module (built-in)
- Regex module (built-in)

---

## ğŸ† Conclusion
This implementation provides a robust **WordPiece Tokenizer** with efficient vocabulary construction, preprocessing, and tokenization. It is designed for **NLP tasks** like embedding models and Neural Language Models.


